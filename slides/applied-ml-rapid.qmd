---
title: "Applied Prediction for Senior Leaders"
format:
  revealjs:
    chalkboard:
        theme: whiteboard
    incremental: true  
    logo: assets/logo.png
    slide-number: true
---

## Today...

::: {.incremental}
1. Approach a real data problem
2. Explore a dataset with Python
3. Build a statistical model
4. See how we did
5. From here to AI!
:::

## From data science to AI?

![](assets/Data_Science_VD.png)

# Adventure awaits!
## Our new business

![](assets/icebreaker.png)

## Our challenge

- A crash is possible...even likely
- We *can* control who is allowed to board
- How did we answer that question with data?

---

![](assets/titanic-kaggle.png)

# Understanding your data
## Getting Started
- The tutorial can be found at bit.ly/ml-cs-board
- We'll be spending most of today looking at [the Kaggle Titanic Dataset](https://www.kaggle.com/competitions/titanic){preview-link="true"}
- So lets take a look at our data!

## Practical 1 - Ingesting and Changing Data
```{.python code-line-numbers="|1|2"}
# importing libraries
import pandas as pd
import seaborn as sns
import statsmodels.formula.api as smf
import statsmodels.api as sm
import matplotlib.pyplot as plt
import numpy as np
```

::: {.notes}
Open notebook and look through until exploration
:::


## Ingesting our data
```{.python}
file_location = '/kaggle/input/titanic/train.csv'

titanic_train = pd.read_csv(file_location, index_col=0)
titanic_train
```

## Thinking about models
::: {.incremental}
- Remember our challenge: we need to identify factors linked to survival
- One of the most inteprettable ways to do this is correlation
- Helping us measure how well two variables are connected
:::

---

![](assets/correlation.svg)

. . .

```{.python}
titanic_train[['Survived','Age']].corr()
```

---

## Practical 2 - Correlations

- Which other factors should we explore?
- How would you interpret your findings?


# The Unreasonable Effectiveness of Linear Regression
## Linear Models
```{python}

import plotly.express as px

df = px.data.tips()
fig = px.scatter(df, x="total_bill", y="tip", trendline="ols")
fig.show()
```

## But we're not restricted to one factor!

```{python}
df = px.data.tips()
fig = px.scatter(df, x="total_bill", y="tip", color="sex", trendline="ols")
fig.show()
```

## What are we doing?
- $y_i = \beta_0 + \kappa T_i + \beta_1 X_{1i} + ... +\beta_k X_{ki} + u_i$
- Really, we're just fitting a line
- But that line can get super, super squiggly
- Machine learning is just using compute to make the best *multidimensional* squiggle

## Practical 3 - Linear Models

# How did we do?

## How good was that, *really*?

- What is the *simplest* prediction you could make?
- How much better did we do?

---

![](assets/accuracy_recall.png)


# But fitting a line isn't AI!

## Text and NLP 
- We haven't used the name category at all
- How would you extract value from it?

---

![](assets/tree4.jpeg)

## Image Recognition
- Thinking beyond tabular data, how could what you've learnt be applied to images?

---

![](assets/mnist-1.png)

---

![](assets/mnist-2.png)

## Evidence House and the AI Digest

![](assets/A2-Evidence-House-Logo-1-1.png)

# Thanks!
::: {.nonincremental}

- avarotsis@no10.gov.uk
- Andreas Varotsis @ Kaggle
:::